{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T14:51:34.623258Z",
     "start_time": "2024-10-02T14:51:34.615719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DOC_AI_LOCATION\"] = \"us\"\n",
    "os.environ[\"DOC_AI_PROCESSOR_ID\"] = \"e977fdd46ee23308\"\n",
    "os.environ[\"PROJECT_ID\"] = \"602280418311\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
    "os.environ[\"LOCATION\"] = \"us-west1\"\n",
    "os.environ['AWS_ACCESS_KEY_ID']=\"\"\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=\"\"\n",
    "os.environ['MLFLOW_TRACKING_URI']=\"https://mlflow-gzpg2zq5pa-uc.a.run.app/\"\n",
    "os.environ['MLFLOW_TRACKING_USERNAME']=\"\"\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD']=\"\"\n",
    "os.environ['LANGCHAIN_TRACING_V2']=\"true\"\n",
    "os.environ['LANGCHAIN_ENDPOINT']=\"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_API_KEY']=\"\"\n",
    "os.environ['LANGCHAIN_PROJECT']=\"iliOS-key-value-extraction\"\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']=''"
   ],
   "id": "214160fb732c6d55",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T14:51:34.634791Z",
     "start_time": "2024-10-02T14:51:34.632653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pathlib\n",
    "os.environ['PYTHONPATH'] = str(pathlib.Path().absolute().parent)"
   ],
   "id": "c3b588d8e4fcd6b2",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T14:51:34.801925Z",
     "start_time": "2024-10-02T14:51:34.800151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from src.doc_ai.processor import DocAIProcessor\n",
    "# from src.doc_ai.processors import DOC_AI_PROCESSOR\n",
    "# from src.pipelines.term_extraction.pipeline_config import Phase1ESAConfig\n",
    "# \n",
    "# config = Phase1ESAConfig()\n",
    "# processor_location: str = os.environ[\"DOC_AI_LOCATION\"]\n",
    "# processor_project_id: str = os.environ[\"PROJECT_ID\"]\n",
    "# processor_id: str = DOC_AI_PROCESSOR[\"PROCESSOR\"]\n",
    "# \n",
    "# processor = DocAIProcessor(\n",
    "#     location=processor_location,\n",
    "#     project_id=processor_project_id,\n",
    "#     processor_id=processor_id,\n",
    "# )\n",
    "# print(config.get_file_names())\n",
    "# file_sequences = []\n",
    "# for file_paths in config.get_file_names():\n",
    "#     file_sequences.append(processor.process_documents(\n",
    "#         [config.get_documents_path() + file_paths]\n",
    "#     ))"
   ],
   "id": "3e80d9b9ad7d68e",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T14:51:34.808441Z",
     "start_time": "2024-10-02T14:51:34.807055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "# with open('file_sequences.pkl', 'wb') as f:\n",
    "#     pickle.dump(file_sequences, f)\n"
   ],
   "id": "9ad389ab13257283",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T14:51:34.814427Z",
     "start_time": "2024-10-02T14:51:34.812644Z"
    }
   },
   "cell_type": "code",
   "source": "path = pathlib.Path().absolute().parent.parent / 'Precomputes/file_sequences_phase_1_esa.pkl'",
   "id": "983ec2a9ce663c9c",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T14:51:36.391432Z",
     "start_time": "2024-10-02T14:51:34.821353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(path, 'rb') as f:\n",
    "    file_sequences = pickle.load(f)"
   ],
   "id": "3f9ee857323a0347",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T14:51:36.400741Z",
     "start_time": "2024-10-02T14:51:36.397535Z"
    }
   },
   "cell_type": "code",
   "source": "file_sequences",
   "id": "a8a1c2e2a9a5f0dc",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-02T14:51:38.725074Z",
     "start_time": "2024-10-02T14:51:36.410026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.pipelines.term_extraction.pipeline_runner import mlflow_metrics\n",
    "from src.validation.validation import calculate_metrics\n",
    "from src.pipelines.term_extraction.utils import get_project_preview, save_results\n",
    "import mlflow\n",
    "import logging.config\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "\n",
    "from src.doc_ai.file_sequence import FileSequence\n",
    "from src.doc_ai.processor import DocAIProcessor\n",
    "from src.gen_ai.gen_ai import get_llm\n",
    "from src.pipelines.constants import NOT_PROVIDED_STR, NOT_SUPPORTED_KEY_ITEM\n",
    "from src.pipelines.term_extraction.pipeline_config import (\n",
    "    Phase1ESAConfig,\n",
    "    PipelineConfig,\n",
    "    PVSystPipelineConfig,\n",
    "    SubscriberMgmtPipelineConfig,\n",
    ")\n",
    "from src.prompts.prompts import (\n",
    "    prompt_template,\n",
    "    prompt_template_instructions,\n",
    "    rag_prompt_template,\n",
    "    rag_prompt_template_pvsyst,\n",
    ")\n",
    "from src.validation.reviewer.reviewer import Reviewer\n",
    "from src.vectordb.vectordb import VectorDB\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Pipeline:\n",
    "    \"\"\"Pipeline for building project previews.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            processor_location: str,\n",
    "            processor_project_id: str,\n",
    "            processor_id: str,\n",
    "            terms_and_definitions: pd.DataFrame,\n",
    "            config: PipelineConfig,\n",
    "            k: int = 10,\n",
    "            chunk_size: Optional[int] = None,\n",
    "            add_tables: bool = True,\n",
    "            few_shot: bool = False,\n",
    "            few_shot_examples: pd.DataFrame = None,\n",
    "            model_type: str = \"CLAUDE\",\n",
    "            file_sequence: FileSequence = None,\n",
    "    ):\n",
    "        \"\"\"Initialize the term_extraction.\"\"\"\n",
    "\n",
    "        self.processor = DocAIProcessor(\n",
    "            location=processor_location,\n",
    "            project_id=processor_project_id,\n",
    "            processor_id=processor_id,\n",
    "        )\n",
    "        self.model_type = model_type\n",
    "        self.llm = get_llm(model_type=self.model_type)\n",
    "        chunk_size = chunk_size if chunk_size is not None else 600\n",
    "        self.vectordb = VectorDB(\n",
    "            k=k,\n",
    "            chunk_size=chunk_size,\n",
    "            add_tables=add_tables,\n",
    "        )\n",
    "        self.retriever: BaseRetriever\n",
    "        self.terms_and_definitions_full = terms_and_definitions.copy()\n",
    "        self.terms_and_definitions = self.terms_and_definitions_preprocess(\n",
    "            terms_and_definitions\n",
    "        )\n",
    "        if few_shot and few_shot_examples is None:\n",
    "            raise ValueError(\"few_shot_examples must be provided if few_shot is True\")\n",
    "        self.few_shot = few_shot\n",
    "        self.few_shot_examples = few_shot_examples\n",
    "        self.reviewer = Reviewer(model_type=self.model_type)\n",
    "        self.config = config\n",
    "        self.file_sequence = file_sequence\n",
    "        self.postprocess_excluded_keys: List[str] | None = None\n",
    "\n",
    "        logger.info(\"Pipeline initialized with the following parameters:\")\n",
    "        logger.info(f\"k: {k}\")\n",
    "        logger.info(f\"few_shot: {few_shot}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def terms_and_definitions_preprocess(\n",
    "            terms_and_definitions: pd.DataFrame,\n",
    "    ) -> pd.DataFrame:\n",
    "        col = (\n",
    "            \"Instructions\"\n",
    "            if \"Instructions\" in terms_and_definitions.columns\n",
    "            else \"Definitions\"\n",
    "        )\n",
    "        terms_and_instructions = terms_and_definitions[\n",
    "            ~terms_and_definitions[col].isna()\n",
    "            & ~terms_and_definitions[\"Key Items\"].isna()\n",
    "            ]\n",
    "        return terms_and_instructions\n",
    "\n",
    "    def get_retriever(self) -> Any:\n",
    "        \"\"\"Get the retriever.\"\"\"\n",
    "        if self.retriever is None:\n",
    "            raise ValueError(\"Retriever has not been built yet.\")\n",
    "        return self.retriever\n",
    "\n",
    "    def _build_prompt(self, term_definition_row: pd.Series) -> str:\n",
    "        \"\"\"\n",
    "        Builds a prompt for a given term and definition.\n",
    "        We can change the prompt template here.\n",
    "        \"\"\"\n",
    "        if \"Instructions\" in term_definition_row:\n",
    "            term = term_definition_row[\"Key Items\"]\n",
    "            instructions = term_definition_row[\"Instructions\"]\n",
    "            prompt = prompt_template_instructions(term, instructions)\n",
    "            return prompt\n",
    "\n",
    "        term = term_definition_row[\"Key Items\"]\n",
    "        definition = term_definition_row[\"Definitions\"]\n",
    "        if self.few_shot and term in self.few_shot_examples.index:\n",
    "            examples = [\n",
    "                example\n",
    "                for example in self.few_shot_examples.loc[term]\n",
    "                if not pd.isna(example) and example != NOT_PROVIDED_STR\n",
    "            ]\n",
    "        else:\n",
    "            examples = []\n",
    "\n",
    "        prompt = prompt_template(term, definition, examples=examples)\n",
    "        return prompt\n",
    "\n",
    "    def _build_a_chain(\n",
    "            self, file_paths: List[str | Path], pages: List[List[int]] | None = None, file_sequence: FileSequence = None\n",
    "    ) -> Any:\n",
    "        \"\"\"\n",
    "        Builds a langchain chain for a given file.\n",
    "        Builds a basic RAG chain for a given file.\n",
    "        \"\"\"\n",
    "        retrieval_qa_chat_prompt = rag_prompt_template()\n",
    "        logger.info(f\"Preparing file for processing: {file_paths}\")\n",
    "        logger.info(f\"Processing pages: {pages}\")\n",
    "        self.retriever = self.vectordb.retriever_from_file_sequence(file_sequence)\n",
    "        combine_docs_chain = create_stuff_documents_chain(\n",
    "            self.llm, retrieval_qa_chat_prompt\n",
    "        )\n",
    "\n",
    "        retrieval_chain = create_retrieval_chain(self.retriever, combine_docs_chain)\n",
    "        return retrieval_chain\n",
    "\n",
    "    def build_project_preview(self, file_paths: List[str | Path], file_sequence: FileSequence) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Main method of the term_extraction. This method builds a project preview for a\n",
    "        given pdf file.\n",
    "        :param file_paths: list of files to bundle together like Site Lease plus\n",
    "            Amendments\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        logger.info(\"Build a default chain\")\n",
    "        chain = self._build_a_chain(file_paths, file_sequence=file_sequence, pages=[[i for i in range(50)]])\n",
    "        logger.info(f\"Building project preview for {file_paths}\")\n",
    "\n",
    "        responses = self.build_responses(chain, file_paths)\n",
    "\n",
    "        logger.info(f\"Finished batch processing for {file_paths}\")\n",
    "\n",
    "        project_preview = pd.DataFrame(self.terms_and_definitions[\"Key Items\"])\n",
    "        project_preview[\"Predicted Legal Terms\"] = responses\n",
    "\n",
    "        project_preview = self.false_positives_postprocessing(project_preview, self.postprocess_excluded_keys)\n",
    "\n",
    "        project_preview[\"Predicted Legal Terms\"] = project_preview[\n",
    "            \"Predicted Legal Terms\"\n",
    "        ].apply(self.remove_triple_backticks)\n",
    "\n",
    "        project_preview = self.terms_and_definitions_full[[\"Key Items\"]].merge(\n",
    "            project_preview, on=\"Key Items\", how=\"left\"\n",
    "        )\n",
    "        project_preview[\"Predicted Legal Terms\"] = project_preview[\n",
    "            \"Predicted Legal Terms\"\n",
    "        ].fillna(NOT_SUPPORTED_KEY_ITEM)\n",
    "\n",
    "        return project_preview\n",
    "\n",
    "    def build_responses(self, chain: Any, file_paths: List[str | Path]) -> List[str]:\n",
    "        prompts = [\n",
    "            self._build_prompt(term_definition_row)\n",
    "            for _, term_definition_row in self.terms_and_definitions.iterrows()\n",
    "        ]\n",
    "        logger.info(f\"Starting batch processing for {file_paths}\")\n",
    "        responses = chain.batch([{\"input\": prompt} for prompt in prompts])\n",
    "        responses = [response[\"answer\"].strip() for response in responses]\n",
    "        return responses  # type: ignore\n",
    "\n",
    "    @staticmethod\n",
    "    def false_positives_postprocessing(project_preview: pd.DataFrame, excluded_keys: List[str] = None) -> pd.DataFrame:\n",
    "        if excluded_keys is None:\n",
    "            excluded_keys = []\n",
    "        project_preview[\"Predicted Legal Terms\"] = project_preview.apply(\n",
    "            lambda row: (\n",
    "                NOT_PROVIDED_STR\n",
    "                if (\n",
    "                           (\"not provided\" in row[\"Predicted Legal Terms\"].lower())\n",
    "                           or (NOT_PROVIDED_STR.lower() in row[\"Predicted Legal Terms\"].lower())\n",
    "                   ) and row[\"Key Items\"] not in excluded_keys\n",
    "                else row[\"Predicted Legal Terms\"]\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        return project_preview\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_triple_backticks(text: str) -> str:\n",
    "        text = text.strip()\n",
    "        if text.startswith(\"```\"):\n",
    "            text = text[3:]\n",
    "        if text.endswith(\"```\"):\n",
    "            text = text[:-3]\n",
    "\n",
    "        return text\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: PipelineConfig) -> \"Pipeline\":\n",
    "        \"\"\"Create a Pipeline instance from a PipelineConfig instance.\"\"\"\n",
    "        return cls(\n",
    "            k=config.k,\n",
    "            chunk_size=config.chunk_size,\n",
    "            few_shot=config.few_shot,\n",
    "            terms_and_definitions=config.get_terms_and_definitions(),\n",
    "            few_shot_examples=config.get_few_shot_examples(),\n",
    "            add_tables=config.add_tables,\n",
    "            processor_location=config.processor_location,\n",
    "            processor_project_id=config.processor_project_id,\n",
    "            processor_id=config.processor_id,\n",
    "            model_type=config.model_type,\n",
    "            config=config,\n",
    "        )\n",
    "\n",
    "class PipelinePhaseIESA(Pipeline):\n",
    "    process_first_n_pages = 5\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.postprocess_excluded_keys = [\"RECs (Y/N and $ amount)\"]\n",
    "\n",
    "    def _build_a_chain_phase_1_esa(\n",
    "            self, file_paths: List[str | Path], pages: List[List[int]] | None = None\n",
    "    ) -> Any:\n",
    "        \"\"\"\n",
    "        Builds a langchain chain for a given file.\n",
    "        Builds a basic RAG chain for a given file.\n",
    "        \"\"\"\n",
    "        retrieval_qa_chat_prompt = rag_prompt_template()\n",
    "        logger.info(f\"Preparing file for processing: {file_paths}\")\n",
    "        logger.info(f\"Processing pages: {pages}\")\n",
    "        try:\n",
    "            file_sequence: FileSequence = self.processor.process_documents(\n",
    "                file_paths, pages=pages\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to create FileSequence. Error: {e}\")\n",
    "        self.retriever = self.vectordb.retriever_from_file_sequence(file_sequence)\n",
    "        combine_docs_chain = create_stuff_documents_chain(\n",
    "            self.llm, retrieval_qa_chat_prompt\n",
    "        )\n",
    "\n",
    "        retrieval_chain = create_retrieval_chain(self.retriever, combine_docs_chain)\n",
    "        return retrieval_chain\n",
    "\n",
    "    def build_responses(self, chain: Any, file_paths: List[str | Path]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Main method of the term_extraction. This method builds a project preview for a\n",
    "        :param file_paths: list of file paths to process\n",
    "        :param chain: default chain\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        logger.info(\"Build a PHASE 1 CHAIN\")\n",
    "        chain_phase_1_esa = self._build_a_chain_phase_1_esa(\n",
    "            file_paths, pages=[[i for i in range(self.process_first_n_pages)]]\n",
    "        )\n",
    "        logger.info(\"Build a SIMPLE CHAIN\")\n",
    "\n",
    "        logger.info(f\"Building project preview for {file_paths}\")\n",
    "\n",
    "        # include only first page of the agreement into the processing\n",
    "        terms_and_definitions_first_page = self.terms_and_definitions[\n",
    "            self.terms_and_definitions[\"Use_first_page\"].values.astype(bool)\n",
    "        ]\n",
    "        # include all the pages of the agreement into the processing\n",
    "        terms_and_definitions_all_pages = self.terms_and_definitions[\n",
    "            ~self.terms_and_definitions[\"Use_first_page\"].values.astype(bool)\n",
    "        ]\n",
    "        prompts_phase_1 = [\n",
    "            self._build_prompt(row)\n",
    "            for _, row in terms_and_definitions_first_page.iterrows()\n",
    "        ]\n",
    "        prompts_other = [\n",
    "            self._build_prompt(row)\n",
    "            for _, row in terms_and_definitions_all_pages.iterrows()\n",
    "        ]\n",
    "        logger.info(f\"Starting batch processing for {file_paths}\")\n",
    "\n",
    "        responses_phase_1_esa = chain_phase_1_esa.batch(\n",
    "            [{\"input\": prompt} for prompt in prompts_phase_1]\n",
    "        )\n",
    "        responses_other = chain.batch([{\"input\": prompt} for prompt in prompts_other])\n",
    "        responses = []\n",
    "        for use_first_page in self.terms_and_definitions[\"Use_first_page\"].tolist():\n",
    "            if use_first_page:\n",
    "                responses.append(responses_phase_1_esa.pop(0))\n",
    "            else:\n",
    "                responses.append(responses_other.pop(0))\n",
    "\n",
    "        responses = [response[\"answer\"].strip() for response in responses]\n",
    "        return responses\n",
    "\n",
    "\n",
    "class PipelineFactory:\n",
    "    \"\"\"Factory for creating pipelines.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def create_pipeline(config: PipelineConfig) -> Pipeline:\n",
    "        \"\"\"Create a pipeline based on the pipeline_name in the config.\"\"\"\n",
    "        if config.pipeline_name == Phase1ESAConfig.pipeline_name:\n",
    "            logger.info(\"Creating Phase I ESA pipeline\")\n",
    "            return PipelinePhaseIESA.from_config(config)\n",
    "        else:\n",
    "            logger.info(\"Creating default pipeline\")\n",
    "            return Pipeline.from_config(config)\n",
    "\n",
    "\n",
    "class PipelineRunner:\n",
    "    \"\"\"Pipeline runner for building project previews.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, pipeline_config: PipelineConfig, experiment: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"Initialize the term_extraction runner.\"\"\"\n",
    "        self.pipeline = PipelineFactory.create_pipeline(pipeline_config)\n",
    "        self.config = pipeline_config\n",
    "        self.override_mlflow_experiment(experiment)\n",
    "        self.result_metrics = None\n",
    "\n",
    "    def run(self, file_sequences: List[FileSequence]) -> None:\n",
    "        \"\"\"Run the term_extraction.\n",
    "\n",
    "        Pipeline steps:\n",
    "        1. Load the correct project preview.\n",
    "        2. Build the project preview using our model.\n",
    "            1.1. Load the document.\n",
    "            1.2. Process the document with DocAI service (extract text).\n",
    "            1.3. Build the RAG system on the provided document text.\n",
    "            1.4. Execute LLM within the RAG system to generate legal terms.\n",
    "            1.5. Build the project preview.\n",
    "        3. Compare the predicted and actual project previews.\n",
    "        4. Calculate the metrics.\n",
    "        5. Save the results.\n",
    "\n",
    "        Note: The term_extraction is run for each file in the term_extraction config.\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(\"STARTING PIPELINE\")\n",
    "        results = []\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            for file_name, file_sequence in zip(self.config.get_file_names(), file_sequences):\n",
    "                logger.info(\"PROCESSING FILE: %s\", file_name)\n",
    "                logger.info(\"PULLING CORRECT PROJECT PREVIEW: %s\", file_name)\n",
    "                correct_project_preview = get_project_preview(\n",
    "                    self.config.get_project_previews_path(), file_name\n",
    "                )\n",
    "                if self.config.pipeline_name == \"pv-syst\":\n",
    "                    correct_project_preview[\"Legal Terms\"] = correct_project_preview[\n",
    "                        \"Value\"\n",
    "                    ].copy()\n",
    "\n",
    "                logger.info(\"BUILDING PROJECT PREVIEW: %s\", file_name)\n",
    "                if type(file_name) is list:\n",
    "                    predicted_project_preview = self.pipeline.build_project_preview(\n",
    "                        [\n",
    "                            self.config.get_documents_path() + attachment\n",
    "                            for attachment in file_name\n",
    "                        ], file_sequence\n",
    "                    )\n",
    "                else:\n",
    "                    predicted_project_preview = self.pipeline.build_project_preview(\n",
    "                        [self.config.get_documents_path() + file_name],  # type: ignore\n",
    "                        file_sequence\n",
    "                    )\n",
    "                logger.info(\"COMPARE: %s\", file_name)\n",
    "                predicted_actual = predicted_project_preview.merge(\n",
    "                    correct_project_preview, on=\"Key Items\", how=\"outer\"\n",
    "                )\n",
    "                logger.info(\"CALCULATE METRICS: %s\", file_name)\n",
    "                metrics = calculate_metrics(predicted_actual, self.config.metrics)\n",
    "                predicted_actual_metrics = predicted_actual.merge(\n",
    "                    metrics, on=\"Key Items\", how=\"outer\"\n",
    "                )\n",
    "                if type(file_name) is list:\n",
    "                    results.append(\n",
    "                        predicted_actual_metrics.assign(file_name=file_name[0])\n",
    "                    )\n",
    "                else:\n",
    "                    results.append(predicted_actual_metrics.assign(file_name=file_name))\n",
    "                logger.info(\"FINISHED PROCESSING: %s\", file_name)\n",
    "            logger.info(f\"SAVING RESULTS TO: {self.config.get_output_results_path()}\")\n",
    "\n",
    "            metrics_total, results_df_name = save_results(\n",
    "                results, self.config.get_output_results_path(), self.config.metrics\n",
    "            )\n",
    "            self.result_metrics = (\n",
    "                metrics_total.iloc[0].drop([\"file_name\", \"key_item\"]).to_dict()\n",
    "            )\n",
    "            mlflow.set_tag(\"Type\", \"Legal Terms Extraction\")\n",
    "            mlflow_metrics(self.config, metrics_total, results_df_name)\n",
    "            logger.info(\"PIPELINE SUCCEEDED\")\n",
    "\n",
    "    def override_mlflow_experiment(self, experiment: str | None) -> None:\n",
    "        \"\"\"Override the mlflow experiment if experiment name is provided\"\"\"\n",
    "        if experiment:\n",
    "            mlflow.set_experiment(experiment)\n",
    "        else:\n",
    "            mlflow.set_experiment(self.config.pipeline_name)\n"
   ],
   "id": "initial_id",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T14:56:56.976430Z",
     "start_time": "2024-10-02T14:51:38.732356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline_config = Phase1ESAConfig()\n",
    "# Run the pipeline\n",
    "pipeline_runner = PipelineRunner(pipeline_config)\n",
    "pipeline_runner.run(file_sequences)"
   ],
   "id": "de4479705aa47c60",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T14:56:56.992243Z",
     "start_time": "2024-10-02T14:56:56.989715Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ee045807dc5d296e",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
