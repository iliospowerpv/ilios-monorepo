{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c84c0b6-f947-48ef-ae40-e8d36cf9c16b",
   "metadata": {
    "tags": []
   },
   "source": [
    "import uuid\n",
    "from typing import Any, List\n",
    "\n",
    "import pandas as pd\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain.text_splitter import CharacterTextSplitter, TextSplitter\n",
    "from langchain_community.retrievers import TFIDFRetriever\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.vectorstores import BaseRetriever\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from src.preprocessing.doc_ai.file_sequence import FileSequence\n",
    "\n",
    "import os\n",
    "os.environ[\"DOC_AI_LOCATION\"] = \"us\"\n",
    "os.environ[\"PROJECT_ID\"] = \"602280418311\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7a84c41-6019-4f9c-90ec-9fa43b17e55a",
   "metadata": {
    "tags": []
   },
   "source": [
    "import os\n",
    "os.environ[\"DOC_AI_LOCATION\"] = \"us\"\n",
    "os.environ[\"PROJECT_ID\"] = \"602280418311\"\n",
    "\n",
    "\n",
    "from src.pipelines.term_extraction.pipeline_config import (\n",
    "    EPCPipelineConfig,\n",
    "    InterconnectionAgreementPipelineConfig,\n",
    "    OMPipelineConfig,\n",
    "    PPAPipelineConfig,\n",
    "    SiteLeasePipelineConfig,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "904db087-06d2-4684-86de-66819de3ec0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "from src.pipelines.term_extraction.pipeline_config import SiteLeasePipelineConfig\n",
    "from src.pipelines.term_extraction.utils import get_project_preview\n",
    "\n",
    "pipeline_config = SiteLeasePipelineConfig()\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76be51ea-be3b-4d1b-ab1c-6f1338a531bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, Image, GenerationConfig\n",
    " \n",
    "PROJECT_ID = \"602280418311\"\n",
    "REGION = \"us-central1\"\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    " \n",
    "model = GenerativeModel(\"gemini-1.5-pro-preview-0409\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b3a4277-d162-4773-baf0-e7dcda1712b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "from typing import Any, Dict, Iterator, List, Mapping, Optional\n",
    "\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.outputs import GenerationChunk\n",
    "\n",
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, Image, GenerationConfig\n",
    " \n",
    "PROJECT_ID = \"602280418311\"\n",
    "REGION = \"us-central1\"\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    " \n",
    "model = GenerativeModel(\"gemini-1.5-pro-preview-0409\")\n",
    "\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "    \"\"\"A custom chat model that echoes the first `n` characters of the input.\n",
    "\n",
    "    When contributing an implementation to LangChain, carefully document\n",
    "    the model including the initialization parameters, include\n",
    "    an example of how to initialize the model and include any relevant\n",
    "    links to the underlying models documentation or API.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            model = CustomChatModel(n=2)\n",
    "            result = model.invoke([HumanMessage(content=\"hello\")])\n",
    "            result = model.batch([[HumanMessage(content=\"hello\")],\n",
    "                                 [HumanMessage(content=\"world\")]])\n",
    "    \"\"\"\n",
    "\n",
    "    model: GenerativeModel\n",
    "    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Run the LLM on the given input.\n",
    "\n",
    "        Override this method to implement the LLM logic.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to generate from.\n",
    "            stop: Stop words to use when generating. Model output is cut off at the\n",
    "                first occurrence of any of the stop substrings.\n",
    "                If stop tokens are not supported consider raising NotImplementedError.\n",
    "            run_manager: Callback manager for the run.\n",
    "            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
    "                to the model provider API call.\n",
    "\n",
    "        Returns:\n",
    "            The model output as a string. Actual completions SHOULD NOT include the prompt.\n",
    "        \"\"\"\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "            \n",
    "        ans = model.generate_content(prompt, \n",
    "                            generation_config=GenerationConfig(max_output_tokens=8000)).candidates[0].content.parts[0].text\n",
    "            \n",
    "        return ans\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[GenerationChunk]:\n",
    "        \"\"\"Stream the LLM on the given prompt.\n",
    "\n",
    "        This method should be overridden by subclasses that support streaming.\n",
    "\n",
    "        If not implemented, the default behavior of calls to stream will be to\n",
    "        fallback to the non-streaming version of the model and return\n",
    "        the output as a single chunk.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to generate from.\n",
    "            stop: Stop words to use when generating. Model output is cut off at the\n",
    "                first occurrence of any of these substrings.\n",
    "            run_manager: Callback manager for the run.\n",
    "            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
    "                to the model provider API call.\n",
    "\n",
    "        Returns:\n",
    "            An iterator of GenerationChunks.\n",
    "        \"\"\"\n",
    "        for char in prompt[: self.n]:\n",
    "            chunk = GenerationChunk(text=char)\n",
    "            if run_manager:\n",
    "                run_manager.on_llm_new_token(chunk.text, chunk=chunk)\n",
    "\n",
    "            yield chunk\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\"\"\"\n",
    "        return {\n",
    "            # The model name allows users to specify custom token counting\n",
    "            # rules in LLM monitoring applications (e.g., in LangSmith users\n",
    "            # can provide per token pricing for their model and monitor\n",
    "            # costs for the given LLM.)\n",
    "            \"model_name\": \"CustomChatModel\",\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model. Used for logging purposes only.\"\"\"\n",
    "        return \"custom\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc69523c-1955-4812-93d8-bc4e76bd3769",
   "metadata": {
    "tags": []
   },
   "source": [
    "llm = CustomLLM(model=model)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1250e917-f6ed-44f7-8362-4453a6b0ac16",
   "metadata": {
    "tags": []
   },
   "source": [
    "llm.batch([\"Hallo\", \"Good bay\"], max_paralellism=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2c4866-a565-41cf-85af-414f4df97d8c",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b34c4c8-cee0-4500-9b9b-86b4c4a4d35e",
   "metadata": {
    "tags": []
   },
   "source": [
    "answers = []\n",
    "\n",
    "\n",
    "for key_item in key_items:\n",
    "    samples = legal_terms[legal_terms[\"Key Items\"] == key_item][\"Legal Terms\"].to_list()\n",
    "    samples = [sample for sample in samples if sample != \"Not provided.\"]\n",
    "    print(key_item)\n",
    "    answer = generate_prompt(samples)\n",
    "    print(\"Answer:\")\n",
    "\n",
    "    print(answer)\n",
    "    answers.append(answer)\n",
    "    print(\"--\"*10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13a37cc1-d1a4-4946-a005-a69ac722e6ae",
   "metadata": {},
   "source": [
    "#A"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a362663f-2b9e-4d7c-b386-833143c59614",
   "metadata": {
    "tags": []
   },
   "source": [
    "pd.DataFrame({\"Key Items\": key_items, \"Instructions\": answers}).to_csv(F\"terms-instructions-{pipeline_config.pipeline_name}.csv\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eb9dec-85a7-4adf-969e-9ac564aaa1c4",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affdf6be-5885-41a2-9149-7e5499ca2980",
   "metadata": {},
   "source": [
    "config"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04b887ad-1f6f-4483-95eb-807fdf210145",
   "metadata": {
    "tags": []
   },
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b65630-c122-43c8-9d63-ff17f6e41e2b",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eeee31-933c-4f81-8b55-cc68a30a4e7d",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a370c36b-0e32-4457-9176-c0d77b855c6c",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae5870e-90d2-43b7-b30d-f15fd5f8ec98",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16722d94-a578-4199-9cd4-68a72b3353a2",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aae9871-2477-45f1-aa0e-9ca38c229bb6",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6d6fba-6811-4450-a7e3-880c3cc291b0",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3368670-8c31-4647-af77-4346d6a6ad4a",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75823c3e-6638-4722-ae69-66ba02e26ef9",
   "metadata": {
    "tags": []
   },
   "source": [
    "from src.preprocessing.doc_ai.processor import DocAIProcessor\n",
    "processor = DocAIProcessor(\n",
    "            location=config.processor_location,\n",
    "            project_id=config.processor_project_id,\n",
    "            processor_id=config.processor_id,\n",
    "        )\n",
    "\n",
    "\n",
    "file_name = config.file_names[1]\n",
    "\n",
    "from src.preprocessing.doc_ai.file_sequence import FileSequence\n",
    "file_sequence: FileSequence = processor.process_documents([config.get_documents_path() + file_name])\n",
    "\n",
    "text = file_sequence.get_all_text()\n",
    "\n",
    "\n",
    "chunk_size = 5000\n",
    "text_splitter = CharacterTextSplitter(\n",
    "            separator=\"\\n\",\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_size // 3,\n",
    "            keep_separator=True,\n",
    "        )\n",
    "\n",
    "docs = text_splitter.create_documents([text])\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"----------------------------------------------\")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-env-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-env-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
